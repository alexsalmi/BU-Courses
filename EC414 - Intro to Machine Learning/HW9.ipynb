{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9: Bayesian Models and Neural Networks\n",
    "\n",
    "By Rachel Manzelli and Brian Kulis with the help of N. Frumkin, K. Chauhan, and A. Tsiligkaridis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Bayesian Models\n",
    "\n",
    "Paleobotanists estimate the moment in the past when a \n",
    "given species became extinct by taking cylindrical, vertical core \n",
    "samples below the earth's surface, and looking for the last\n",
    "occurrence of the species in the fossil record. This is measured in meters\n",
    "above the point $P$ at which the species was known to have first\n",
    "emerged.  \n",
    "\n",
    "Letting $\\{y_i, i = 1, \\ldots, n\\}$ denote a sample of\n",
    "such distances above $P$ at a random set of locations, the model can be represented as\n",
    "\n",
    "\\begin{equation*}\n",
    "(y_i | \\theta) \\sim \\mbox{Unif}(0,\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "In this model the unknown $\\theta > 0$ can be used to estimate\n",
    "the species extinction time through carbon dating.  *This problem is about Bayesian inference for $\\theta$, and it will be seen that some of our usual intuitions do not quite hold in this case.*\n",
    "\n",
    "**a.** Show that the likelihood may be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "l(\\theta | y) = \n",
    "\\theta^{-n} I(\\theta \\geq \\max(y_1, \\ldots, y_n)),\n",
    "\\end{equation*}\n",
    "\n",
    "where $I$ is the **indicator function**, i.e. $I(A) = 1 $ if $A$ is true and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We know that \n",
    "\n",
    "$l(\\theta | y_i) = p(y_i | \\theta) p(\\theta)$\n",
    "\n",
    "We also know that \n",
    "\n",
    "$p(y_i | \\theta) = \\sum_{i=0}^{n} 1/\\theta = 1/\\theta^n = \\theta^{-n}$, since this is a uniform distribution from 0 to $\\theta$\n",
    "\n",
    "We also know that for $p(y_i | \\theta)$ to be a valid uniform distribution, that:\n",
    "\n",
    "$(0<= y_i <= \\theta$ for all $i=1,...,n)$\n",
    "\n",
    "If this is not true, then $p(y_i | \\theta)$ is not a valid uniform distribution, and $l(\\theta | y_i) = 0$\n",
    "\n",
    "$(0<= y_i <= \\theta$ for all $i=1,...,n)$ can be rewritten as:\n",
    "\n",
    "$(0<= y_1, y_2, ..., y_n <= \\theta)$, or equivilently:\n",
    "\n",
    "$(0<= max(y_1,...,y_n) <= \\theta)$\n",
    "\n",
    "Since we now have that if $(\\theta >= max(y_1,...,y_n))$ is false, then $l(\\theta | y_i) = 0$, and if it is true, then $l(\\theta | y_i) = p(y_i | \\theta)*(1) = \\theta^{-n}$\n",
    "\n",
    "Therefore, we can write the likelihood function as:\n",
    "\n",
    "$l(\\theta | y_i) = \\theta^{-n}I(\\theta \\geq \\max(y_1, \\ldots, y_n))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** The Pareto distribution (written $\\theta \\sim \\mbox{Pareto}(\\alpha,\\beta)$\n",
    "has density/PDF:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\alpha \\beta^\\alpha \\theta^{-(\\alpha + 1)} & \\mbox{if}\\ \\theta \\geq \\beta \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array},\n",
    "\\right .\n",
    "\\end{equation*}\n",
    "where $\\alpha, \\beta > 0$.\n",
    "\n",
    "With the likelihood viewed as a constant multiple of a\n",
    "density for $\\theta$, show that the likelihood corresponds to\n",
    "the $\\mbox{Pareto}(n-1,m)$ distribution, *where you will need to determine m.*\n",
    "\n",
    "Now, let the prior\n",
    "for $\\theta$ be taken to be $\\mbox{Pareto}(\\alpha,\\beta)$\n",
    "and derive the posterior distribution $p(\\theta | y)$.\n",
    "\n",
    "Is the Pareto conjugate to the uniform? As discussed in class, a likelihood and prior are conjugate pairs if the posterior distribution is in the same class (in this case, a Pareto distribution) as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Since the likelihood is:\n",
    "\n",
    "$l(\\theta | y_i) = \\theta^{-n}I(\\theta \\geq \\max(y_1, \\ldots, y_n))$\n",
    "\n",
    "It corresponds to a Pareto distribution i the sense that it is a constant (1) multiplied by theta to the power of a negative exponent, the exponent in this case being -n. So to find the value for m, we need to solve the constant term for $\\beta$\n",
    "\n",
    "$\\alpha\\beta^\\alpha = 1$, where $\\alpha = n-1$\n",
    "\n",
    "$(n-1)\\beta^{-(n-1)} = 1$\n",
    "\n",
    "$\\beta^{-(n-1)} = 1/(n-1)$\n",
    "\n",
    "$\\beta^{n-1} = n-1$\n",
    "\n",
    "$\\beta = (n-1)^{-(n-1)}$\n",
    "\n",
    "Giving us $m = (n-1)^{-(n-1)}$, and showing us that the likelihood is a Pareto($n-1$,$(n-1)^{-(n-1)}$) distribution. \n",
    "\n",
    "To find the posterior, we simply perform the calculation:\n",
    "\n",
    "$p(\\theta | y) = p(y | \\theta) p(\\theta)$\n",
    "\n",
    "\\begin{equation*}\n",
    "= \\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\alpha \\beta^\\alpha \\theta^{-(\\alpha + 1)} \\theta^{-n} & \\mbox{if}\\ \\theta \\geq \\beta \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array},\n",
    "\\right .\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "= \\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\alpha \\beta^\\alpha \\theta^{-(\\alpha + n + 1)} & \\mbox{if}\\ \\theta \\geq \\beta \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array},\n",
    "\\right .\n",
    "\\end{equation*}\n",
    "\n",
    "This is a conjucate pair, since this derived posterior is still a Pareto distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** In an experiment conducted in the Antarctic in the\n",
    "1980's to study a particular species of fossil ammonite,\n",
    "the following was a linearly rescaled version of the data\n",
    "obtained, in ascending order: \n",
    "\n",
    "$y = (0.4, 1.0, 1.5, 1.7, 2.0, 2.1, 3.1, 3.7, 4.3, 4.9)$.  \n",
    "\n",
    "Prior information equivalent to a Pareto prior with $(\\alpha,\\beta) = (2.5,4)$ was available. Plot the prior, likelihood, and posterior distributions arising from this data set on the same graph, and briefly discuss what this picture implies about the updating of information from prior to posterior in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Updating from prior to posterior lowers the values on the graph, closer to what the actual likelihood. Other than that, the distributions have the same general shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmUVNW59/Hv09WTqAxia5RZpJupSoZ2BI03GiW+XodEEzRRHFaIc/RNouK7Vq43WUlITJyi4nVAoqLexCmsG2JUkhtDHBvFZm4BEZsgtogMCvT0vH+cU03RA13dXd3VXfX7uGrVqX323vV0ITx9zt61t7k7IiKSnXLSHYCIiKSPkoCISBZTEhARyWJKAiIiWUxJQEQkiykJiIhkMSUBEZEspiQgIpLFlARERLJYbroDaM3BBx/sQ4cOTXcYIiI9xqJFiz5x96Jk6nb7JDB06FDKysrSHYaISI9hZh8kW1e3g0REspiSgIhIFms1CZjZIDP7m5ktN7NlZvb9sPwgM3vJzN4Ln/uF5WZmd5vZajMrN7MJCX1NC+u/Z2bTOu/HEhGRZCQzJlAL/MDd3zazA4FFZvYScAmwwN1nmtnNwM3ATcDXgBHh41hgFnCsmR0E/AdQCnjYzzx335LqH0pEuq+amhoqKyvZtWtXukPp8QoLCxk4cCB5eXnt7qPVJODuG4GN4fF2M1sBDADOBk4Oq/0O+F+CJHA28KgHGxW8bmZ9zeywsO5L7v4pQJhIpgBPtjt6EelxKisrOfDAAxk6dChmlu5weix3Z/PmzVRWVjJs2LB299OmMQEzGwqMB94ADg0TBMBHwKHh8QDgw4RmlWFZS+UikkV27dpF//79lQA6yMzo379/h6+okk4CZnYA8AxwvbtvSzwX/tafsi3KzGy6mZWZWVlVVVWquhWRbkIJIDVS8TkmlQTMLI8gAcx192fD4k3hbR7C54/D8g3AoITmA8OylsqbcPcH3L3U3UuLipL6vsPeampg5kx46aW2txURySLJzA4y4GFghbvfnnBqHhCf4TMN+GNC+cXhLKHjgK3hbaO/AKeZWb9wJtFpYVnq5ebCbbfB0093Svci0rMdcMABAPzrX//ivPPOA2DOnDlcc801SfeRWP/+++/n0UcfBeDkk0/ulC+4dla/ycwOmgRcBCwxs8Vh2S3ATOD3ZnY58AHwzfDcfOAMYDXwBXApgLt/amY/Bd4K6/0kPkiccmYQjUJ5ead0LyKZ4fDDD+fpFPyyeMUVV6QgmvRo9UrA3Re6u7l7zN3HhY/57r7Z3U9x9xHufmr8H3QPXO3uw9096u5lCX3Ndvcjw8cjnfmDEYvB0qVQX9+pbyMiPde6desYO3Zsk/I//elPHH/88XzyySdUVVXxjW98g6OPPpqjjz6af/7zn03q33rrrfz6179ueP2HP/yBY445huLiYv7xj38AwYD4pZdeSjQaZfz48fztb3/bZ/nOnTuZOnUqo0aN4txzz2Xnzp2d8RF0/7WD2i0Wgx07YN06OOKIdEcjIs24/npYvLj1em0xbhzceWf72z/33HPcfvvtzJ8/n379+nHhhRdyww03MHnyZNavX8/pp5/OihUr9tlHbW0tb775JvPnz+c///M/efnll7n33nsxM5YsWcLKlSs57bTTqKioaLF81qxZ9OrVixUrVlBeXs6ECRP2+Z7tlblJIBoNnsvLlQREJCl//etfKSsr48UXX6R3794AvPzyyyxfvryhzrZt29ixY8c++/n6178OwMSJE1m3bh0ACxcu5NprrwVg5MiRDBkyhIqKihbLX3nlFa677joAYrEYsVgspT9rXOYmgTFjgrGBJUvgnHPSHY2INKMjv7F3huHDh7N27VoqKiooLS0FoL6+ntdff53CwsKk+ykoKAAgEolQW1vbKbGmSuYuIHfAATB8uAaHRSRpQ4YM4ZlnnuHiiy9m2bJlAJx22mn89re/baizuJ33r0488UTmzp0LQEVFBevXr6ekpKTF8pNOOoknnngCgKVLl1LeSf+WZW4SAM0QEpE2GzlyJHPnzuX8889nzZo13H333ZSVlRGLxRg9ejT3339/u/q96qqrqK+vJxqN8q1vfYs5c+ZQUFDQYvmVV17Jjh07GDVqFD/+8Y+ZOHFiin/SgAVf9u2+SktLvd1zY2+9FX76U9i+HXr1SmlcItI+K1asYNSoUekOI2M093ma2SJ3L02mfeZfCdTXQ8KgjoiI7JHZSSA+mr5kSXrjEBHppjI7CRxxRHAbSOMCIiLNyuwkEIkEU0WVBEREmpXZSQCCW0Ll5dDNB8BFRNIhO5LAJ5/Apk3pjkREpNvJ/CSQuHyEiEgb/PjHP+bll19OdxidKnOXjYiLJ4ElS+C009Ibi4j0GHV1dfzkJz9pc5tIJNJJEXWOzL8SOPhgOPxwXQmISIN169YxcuRIvv3tbzNq1CjOO+88vvjiC4YOHcpNN93EhAkT+MMf/sAll1zSsN/AggULGD9+PNFolMsuu4zdu3cDNGnT02T+lQBo+QiR7iqNa0mvWrWKhx9+mEmTJnHZZZdx3333AdC/f3/efvttAF544QUgWPP/kksuYcGCBRQXF3PxxRcza9Ysrr/++iZteppktpecbWYfm9nShLL/NrPF4WNdfMcxMxtqZjsTzt2f0GaimS0xs9Vmdrd15U7TsVjwreFuvpqfiHSdQYMGMWnSJAC+853vsHDhQgC+9a1vNam7atUqhg0bRnFxMQDTpk3jlVdeaTjfXJueIpkrgTnAPcCj8QJ3b/iJzew3wNaE+mvcfVwz/cwCvgu8QbAF5RTgz20PuR2iUaiuhooKGD26S95SRJKQxrWkG/8eGn+9//77t7mv9rTpLpLZXvIVoNm9gMPf5r8JPLmvPszsMKC3u7/uwYp1jwJdt8i/lo8QkUbWr1/Pa6+9BsATTzzB5MmTW6xbUlLCunXrWL16NQCPPfYYX/7yl7skzs7W0YHhE4FN7v5eQtkwM3vHzP5uZieGZQOAyoQ6lWFZ1xg5EnJzNS4gIg1KSkq49957GTVqFFu2bOHKK69ssW5hYSGPPPII559/PtFolJycnB69uXyijg4MX8DeVwEbgcHuvtnMJgLPm9mYtnZqZtOB6QCDBw/uYIhAQQGUlCgJiEiD3NxcHn/88b3K4ltBxs2ZM6fh+JRTTuGdd95p0k/jNj1Nu68EzCwX+Drw3/Eyd9/t7pvD40XAGqAY2AAMTGg+MCxrlrs/4O6l7l5aVFTU3hD3FovpdpCISCMduR10KrDS3Rtu85hZkZlFwuMjgBHAWnffCGwzs+PCcYSLgT924L3bLhaDDz6ArVtbrysiGW3o0KEsXbq09YpZIJkpok8CrwElZlZpZpeHp6bSdED4JKA8nDL6NHCFu8cHla8CHgJWE1whdM3MoLjEbw6LiAiQxJiAu1/QQvklzZQ9AzzTQv0yYGwb40udxBlC+5gFICKSTTJ/2Yi4gQOhb18NDouIJMieJGCm5SNERBrJniQAe2YIaYMZEemg559/nuXLl7e53bx585g5c2YnRNQ+2ZUEolHYvj2YJSQi0gHtSQK1tbWcddZZ3HzzzW1q05myKwlo+QgRoeWlpFtaLvrmm29m9OjRxGIxfvjDH/Lqq68yb948fvSjHzFu3DjWrFnDmjVrmDJlChMnTuTEE09k5cqVAFxyySVcccUVHHvssdx4443MmTOHa665piGOr3zlK8RiMU455RTWr1/fbJvOlB1LSceNDScnlZfDv/97emMREa5/4XoWf5TapaTHfWkcd05p+1LSt99+O//1X//VZLnoiy66iOeee46VK1diZnz22Wf07duXs846izPPPJPzzjsPCL5RfP/99zNixAjeeOMNrrrqKv76178CUFlZyauvvkokEtnrW8jXXnst06ZNY9q0acyePZvrrruO559/vkmbzpRdVwIHHgjDhmlwWESaLCW9YMGCZpeL7tOnD4WFhVx++eU8++yz9OrVq0lfO3bs4NVXX+X8889n3LhxfO9732Pjxo0N588///xm/zF/7bXXuPDCCwG46KKLGpaz3lebVMuuKwHQ8hEi3Ugyv7F3lsZLSfft25fNmzc3qZebm8ubb77JggULePrpp7nnnnsafsOPq6+vp2/fvixuYYOc7rw8dXZdCUCQBFatgl270h2JiKRR46WkS0tLm10ueseOHWzdupUzzjiDO+64g3fffReAAw88kO3btwPQu3dvhg0b1rC9pLs31NuXE044gaeeegqAuXPncuKJJ7bSIvWyLwlEo1BfH+w0JiJZq/FS0jfccEOzy0Vv376dM888k1gsxuTJk7n99tsBmDp1Krfddhvjx49nzZo1zJ07l4cffpijjjqKMWPG8Mc/tr482m9/+1seeeQRYrEYjz32GHfddVdn/9hNmHfzOfOlpaVeVlaWug5XrQr2F5gzB6ZNS12/IpKUFStWMGrUqLTGsG7dOs4888yMWESuuc/TzBa5e2ky7bPvSuDII6GwUIPDIiJkYxKIRGDMGCUBkSympaT3yL4kAJohJJJm3f02dE+Ris8xO5NANAqbNgUPEelShYWFbN68WYmgg9ydzZs3U1hY2KF+su97ArD38hGHHpreWESyzMCBA6msrKSqqirdofR4hYWFDBw4sPWK+9BqEjCz2cCZwMfuPjYsuxX4LhD/U7zF3eeH52YAlwN1wHXu/pewfApwFxABHnL39C2jl5gETj01bWGIZKO8vDyGDRuW7jAklMztoDnAlGbK73D3ceEjngBGE2w7OSZsc5+ZRcJ9h+8FvgaMBi4I66ZHUVFwBaDBYRHJcslsL/mKmQ1Nsr+zgafcfTfwvpmtBo4Jz61297UAZvZUWDd939jS4LCISIcGhq8xs3Izm21m/cKyAcCHCXUqw7KWytMnFoNly6CT1+oWEenO2psEZgHDgXHARuA3KYsIMLPpZlZmZmWdNngUjQbrB4XrhIiIZKN2JQF33+Tude5eDzzInls+G4BBCVUHhmUtlbfU/wPuXurupUVFRe0JsXXaYEZEpH1JwMwOS3h5LhD/6t08YKqZFZjZMGAE8CbwFjDCzIaZWT7B4PG89oedAqNGBd8e1uCwiGSxZKaIPgmcDBxsZpXAfwAnm9k4wIF1wPcA3H2Zmf2eYMC3Frja3evCfq4B/kIwRXS2uy9L+U/TFoWFUFysJCAiWS37VhFNNHUqvPkmrF3bOf2LiKSBVhFNVjQK778P27alOxIRkbTI7iQQHxzWaoIikqWUBEAzhEQka2V3Ehg8GHr31uCwiGSt7E4CZsG4gK4ERCRLZXcSgOCWUHk5dPNZUiIinUFJIBqFrVvhww9brysikmGUBDQ4LCJZTElg7NjgWYPDIpKFlAT69IEhQ5QERCQrKQmANpgRkaylJADB4PDKlbB7d7ojERHpUkoCEFwJ1NXBihXpjkREpEspCYBmCIlI1lISABgxAgoKNDgsIllHSQAgNxdGj1YSEJGs02oSMLPZZvaxmS1NKLvNzFaaWbmZPWdmfcPyoWa208wWh4/7E9pMNLMlZrbazO42M+ucH6mdNENIRLJQMlcCc4ApjcpeAsa6ewyoAGYknFvj7uPCxxUJ5bOA7xLsOzyimT7TKxqFjRvhk0/SHYmISJdpNQm4+yvAp43KXnT32vDl68DAffURbkzf291f92A/y0eBc9oXcifR4LCIZKFUjAlcBvw54fUwM3vHzP5uZieGZQOAyoQ6lWFZ9xFPAhoXEJEsktuRxmb2/4BaYG5YtBEY7O6bzWwi8LyZjWlHv9OB6QCDBw/uSIjJO/RQKCpSEhCRrNLuKwEzuwQ4E/h2eIsHd9/t7pvD40XAGqAY2MDet4wGhmXNcvcH3L3U3UuLioraG2LbaXBYRLJMu5KAmU0BbgTOcvcvEsqLzCwSHh9BMAC81t03AtvM7LhwVtDFwB87HH2qRaPBpvN1demORESkSyQzRfRJ4DWgxMwqzexy4B7gQOClRlNBTwLKzWwx8DRwhbvHB5WvAh4CVhNcISSOI3QPsRjs3Alr1qQ7EhGRLtHqmIC7X9BM8cMt1H0GeKaFc2XA2DZF19USZwgVF6c3FhGRLqBvDCcaPRpycjQ4LCJZQ0kg0X77BesIKQmISJZQEmhMM4REJIsoCTQWjQYDwzt2pDsSEZFOpyTQWHxweNmy9MYhItIFlAQa0/IRIpJFlAQaGzIEDjhASUBEsoKSQGM5OcG4gAaHRSQLKAk0JxoNrgSCJZFERDKWkkBzYjHYsgU2tLjGnYhIRlASaI42mBGRLKEk0Jyx4RJHGhwWkQynJNCcfv1g0CAlARHJeEoCLdHyESKSBZQEWhKNwooVUF2d7khERDqNkkBLYjGorYWVK9MdiYhIp0kqCZjZbDP72MyWJpQdZGYvmdl74XO/sNzM7G4zW21m5WY2IaHNtLD+e2Y2LfU/TgpphpCIZIFkrwTmAFMald0MLHD3EcCC8DXA1wj2Fh4BTAdmQZA0gP8AjgWOAf4jnji6peJiyMvT4LCIZLSkkoC7vwJ82qj4bOB34fHvgHMSyh/1wOtAXzM7DDgdeMndP3X3LcBLNE0s3UdeXrDTmK4ERCSDdWRM4FB33xgefwQcGh4PAD5MqFcZlrVU3n3Fl48QEclQKRkYdncHUrbQjplNN7MyMyurqqpKVbdtF4sFS0d82vgiSEQkM3QkCWwKb/MQPn8clm8ABiXUGxiWtVTehLs/4O6l7l5aVFTUgRA7SIPDIpLhOpIE5gHxGT7TgD8mlF8czhI6Dtga3jb6C3CamfULB4RPC8u6r2g0eNYtIRHJULnJVDKzJ4GTgYPNrJJgls9M4PdmdjnwAfDNsPp84AxgNfAFcCmAu39qZj8F3grr/cTdu/d9lsMOg/79lQREJGMllQTc/YIWTp3STF0Hrm6hn9nA7KSjSzczLR8hIhlN3xhuTXyXsfr6dEciIpJySgKticXgiy9g7dp0RyIiknJKAq3RDCERyWBKAq0ZMyYYG9DgsIhkICWB1vTqBUceqSsBEclISgLJ0PIRIpKhlASSEYvB6tXw+efpjkREJKWUBJIRi4E7LF+e7khERFJKSSAZWj5CRDKUkkAyjjgiGCBWEhCRDKMkkIycnD3fHBYRySBKAsmKzxDylG2bICKSdkoCyYrFYPNm2Lix9boiIj2EkkCytHyEiGSgjE0C9V5PTV1N6jrUDCERyUAZmQS27trKUfcfxd1v3J26Tg86CAYMUBIQkYzS7iRgZiVmtjjhsc3MrjezW81sQ0L5GQltZpjZajNbZWanp+ZHaKpPYR8O2f8QfvPab9hduzt1HWuGkIhkmHYnAXdf5e7j3H0cMJFgK8nnwtN3xM+5+3wAMxsNTAXGAFOA+8ws0rHwW3bL5FvYuGMjv3v3d6nrNBYLvjVck8LbTCIiaZSq20GnAGvc/YN91DkbeMrdd7v7+wR7EB+Tovdv4ivDvsLRhx/NL//5S2rra1PTaSwWJICKitT0JyKSZqlKAlOBJxNeX2Nm5WY228z6hWUDgA8T6lSGZZ3CzLjlxFtYu2Utv1/2+9R0qsFhEckwHU4CZpYPnAX8ISyaBQwHxgEbgd+0o8/pZlZmZmVVVVXtju2skrMYXTSaXyz8BfWegj2CR46E3FwlARHJGKm4Evga8La7bwJw903uXufu9cCD7LnlswEYlNBuYFjWhLs/4O6l7l5aVFTU7sByLIcZk2ew9OOl/KniT+3up0F+PowapcFhEckYqUgCF5BwK8jMDks4dy6wNDyeB0w1swIzGwaMAN5Mwfvv09SxUxnadyg/+8fP8FQs+aANZkQkg3QoCZjZ/sBXgWcTin9lZkvMrBz4N+AGAHdfBvweWA68AFzt7nUdef9k5ObkcuMJN/LGhjf433X/2/EOYzH48EPYsqXjfYmIpFmHkoC7f+7u/d19a0LZRe4edfeYu5/l7hsTzv3M3Ye7e4m7/7kj790Wl46/lEP3P5SfL/x5xzuLDw4vXbrveiIiPUBGfmO4scLcQn5w/A94ee3LvLXhrY51Fl9DSLeERCQDZEUSALii9Ar6FvblFwt/0bGOBgyAfv2UBEQkI2RNEjiw4ECuO+Y6nlv5HMurOrBXsJmWjxCRjJE1SQDgumOvo1deL2YunNmxjmKxIAnUp+C7ByIiaZRVSaB/r/58b+L3eGLJE7y/5f32dxSLwY4d8MG+VskQEen+sioJAPzg+B+QYznc9upt7e9Ey0eISIbIuiQwoPcALhl3CbPfmc1HOz5qXydjxwbPSgIi0sNlXRIAuHHSjdTU13DHa3e0r4MDDoDhwzU4LCI9XlYmgSMPOpJvjvkm95Xdx5ad7fzmr5aPEJEMkJVJAGDG5BnsqN7BPW/e074OYjF47z3YuTO1gYmIdKGsTQKxQ2OcWXwmd75xJzuqd7S9g2g0mCK6vAPfORARSbOsTQIQbEH56c5PeXDRg21vrOUjRCQDZHUSOH7Q8Zw89OT2bUg/fDjst5+SgIj0aFmdBCAYG9iwfQOPlT/WtoaRCIwZoxlCItKjZX0S+OoRX2XiYROZuXBm2zekj8Xg3XchFZvViIikQdYngfiG9Gu2rOHp5U+3rXEsBp98Aps2dU5wIiKdLBUbza8LdxJbbGZlYdlBZvaSmb0XPvcLy83M7jaz1WZWbmYTOvr+qXDOyHMYefBIfrHwF23bgjK+fIRuCYlID5WqK4F/c/dx7l4avr4ZWODuI4AF4WsINqUfET6mA7NS9P4dEt+QvnxTOfPfm598Q60hJCI9XGfdDjob+F14/DvgnITyRz3wOtC30cb0aXPB2AsY0mdI2zakLyqCww7TlYCI9FipSAIOvGhmi8xselh2aMLewh8Bh4bHA4APE9pWhmVplxfJ40cn/IjXKl/jlQ9eSb6hlo8QkR4sFUlgsrtPILjVc7WZnZR40oNfq9s0fcbMpptZmZmVVVVVpSDE5Fw2/jIO2f+Qtm1IH4sF3xqubePMIhGRbqDDScDdN4TPHwPPAccAm+K3ecLnj8PqG4BBCc0HhmWN+3zA3UvdvbSoqKijISZtv7z9+L/H/V9eXPMii/61KLlG0Sjs3h2sIyQi0sN0KAmY2f5mdmD8GDgNWArMA6aF1aYBfwyP5wEXh7OEjgO2Jtw26hauPPpK+hT0SX5Dei0fISI9WEevBA4FFprZu8CbwJ/c/QVgJvBVM3sPODV8DTAfWAusBh4Erurg+6dc74LeXHvMtTy74llWVK1ovcGoUcG3h5UERKQHsjbNi0+D0tJSLysr69L3rPq8iiF3DuGbY77JnHPmtN5gzJhgLaF58zo9NhGR1pjZooQp+/uU9d8Ybk7R/kVMnzidx8sfZ91n61pvEIvpSkBEeiQlgRbEN6T/9au/br1yLAYffABbt3Z+YCIiKaQk0IJBfQZx8VEX8/A7D7NpRytrA8W/Obx0aecHJiKSQkoC+3DTpJuorqvmztfv3HdFzRASkR5KSWAfRvQfwXmjz+Pet+7ls12ftVxx0CDo00fLR4hIj6Mk0IoZk2ewvXo79755b8uVzLR8hIj0SEoCrRj3pXGcMeIM7nzjTr6o+aLlirFYcCXQzafciogkUhJIwi2Tb+GTLz7hobcfarlSNArbtsH69V0XmIhIBykJJGHS4EmcNOQkbnv1NqrrqpuvpMFhEemBlASSNGPyDCq3VfJ4+ePNVxg7NnhWEhCRHkRJIEmnDz+d8V8az8yFM6mrr2taoXdvGDpUM4REpEdREkhSfEP69z59j2dWPNN8JS0fISI9jJJAG5w78lxK+pe0vCF9LAYVFbBrV9cHJyLSDkoCbRDJiXDz5JtZ/NFiXlj9QtMK0SjU1cGKJJagFhHpBpQE2ujC6IUM6j2o+S0oNUNIRHoYJYE2yo/k86MTfsTC9Qv5xwf/2PvkkUdCYaGSgIj0GO1OAmY2yMz+ZmbLzWyZmX0/LL/VzDaY2eLwcUZCmxlmttrMVpnZ6an4AdLh8gmXU9SrqOnVQG4ujB6tGUIi0mN05EqgFviBu48GjgOuNrPR4bk73H1c+JgPEJ6bCowBpgD3mVmkA++fNr3yenHDcTfwwuoXeGfjO3uf1AwhEelB2p0E3H2ju78dHm8HVgAD9tHkbOApd9/t7u8T7DN8THvfP92uOvoqehf0brohfTQKmzbBxx+nJzARkTZIyZiAmQ0FxgNvhEXXmFm5mc02s35h2QDgw4Rmlew7aXRrfQr7cPXRV/P08qdZ9cmqPSfig8O6JSQiPUCHk4CZHQA8A1zv7tuAWcBwYBywEfhNO/qcbmZlZlZWVVXV0RA7zfXHXU9BbgG//Ocv9xRqhpCI9CAdSgJmlkeQAOa6+7MA7r7J3evcvR54kD23fDYAgxKaDwzLmnD3B9y91N1Li4qKOhJipzpk/0P47oTv8lj5Y6zfGq4eesghwUNXAiLSA3RkdpABDwMr3P32hPLDEqqdC8Q33p0HTDWzAjMbBowA3mzv+3cXPzzhhwD85tWECx4NDotID9GRK4FJwEXAVxpNB/2VmS0xs3Lg34AbANx9GfB7YDnwAnC1uzezElvPMrjPYC6KXcSDbz/Ix5+Hg8GxGCxbFnx7WESkG+vI7KCF7m7uHkucDuruF7l7NCw/y903JrT5mbsPd/cSd/9zan6E9Ltp0k3sqt3FXa/fFRREo8H6QatXpzcwEZFW6BvDKVBycAnfGP0N7nnrHrbu2qrBYRHpMZQEUmTG5Bls272N+966L/jWcE6OkoCIdHtKAiky4bAJTDlyCne8fgdfROqhuFgzhESk21MSSKFbJt9C1RdVzH5ntmYIiUiPoCSQQicOOZHJgyfzq3/+iuqxo+D992H79nSHJSLSIiWBFJsxeQYfbvuQJwZuCQqWLt13AxGRNFISSLGvHfk1jjr0KGZ+9j/UGbolJCLdmpJAisU3pF+1bS3Pjy/U4LCIdGtKAp3gG6O+wYiDRvDzkyN4+bvpDkdEpEVKAp0gviH9270/58Wtb4N7ukMSEWmWkkAn+U7sOwzM6cvPJ3wBlZXpDkdEpFm56Q4gU+VH8vnh8Iu5vv5uzvr1REb3H0nxoHGUjDmJ4tEncfD+RQQLsYqIpI95N79VUVpa6mWk5JA8AAAIN0lEQVRlZekOo12+2LGFa39cymv+IasPrKEmYUflfjW5lHh/insPpWTgURSXTKJk0HiOPOhI9svbL31Bi0iPZ2aL3L00qbpKAl2jdusW1pW9TMWyv7Nq/WIqtq5hVX0Vq/rW8a/ee+qZw2D6UHLAUIoPj1JyxNEUF42kpH8Jg/oMIsd0B09E9k1JoKdwh/Xr2fHOG1Qs/TsVH7zNqi2rWcUnVPSHVf1hR8Ge6oXkMWL/QZR8aSzFh42l5OASSvqXUNy/mH779Wv5fUQkqygJ9HQ7d8KKFfi77/LR0tdYtW4RFZsrWFWwg1UHQ0V/WNsP6hIuCooK+lFcNIqSg0dS3L+4IUEc0e8ICnILWn4vEck43ToJmNkU4C4gAjzk7jP3VT8rk0Bz3GHTpuAbyOXlVJe/w/trFwVXDn3rgiuHIqPikAibCmsbmuVYDsP6DuOIfkfQK68X+ZH8pB95OXltqp8fyScv0rRNxCIaBBfpQt02CZhZBKgAvgpUAm8BF7j78pbaKAm0oqYGVq1qSA4sWcJnKxdTsftfDbeUKg7P5/1D8tmdC9U5UJ3jVOfUU21OtdVTY/UNz53BsCAh5OSFSaKAvEheQ8KIJ5u8SF7rxzntbNfoODcnF8MwMwwjx3Iajs3C12043542OZZDbk4u+ZH8IB4lSkmRtiSBrp4iegyw2t3XApjZU8DZBPsOS3vk5cHYscHjwgsB6Asc8+mnHLNkyZ7ksKgi2PKypmbvR3V1w7HXVFNTV0N1XXWQKCI0+6jJab682boRqI441ZHdVEd2szssq8kJz+UaNRGjJtfCY9gVMbbF60WCxNXwnOPU5DjVOU6NBceZIp9c8i2XPIuQb3nk5eSSb3nhcR75OeFxJI88yyMvJ5+8SD75OfkNx3mRguA4N35cQF5uYXCcW0BupJC8SCF5eYXk5uSTZ/nk5uQHEw58T6IiTFJ4PGnlEKTz5s8Z1tCeZuo0nHMDM3IS+ou394S6AQvLaXjf4JmGuonnghPW0Ne++gh+923Uhwfn3dmrfE9b9joXfx3XuCyZc/uqU1AAp56a/P8/7dXVSWAA8GHC60rg2M54o/Hjg1vrrX34bfmDSvUfclvOt73NQbh/Gfhyk/ONNTmXA+b15NbUkFdTQx415FNNHjXkek3Dc7xsr4eH9aghz/dum0cNB3gNudQ2PCJeu/frhOPgffY+37hOhBospwYitXhODUSq8Ugt5NRQH6nFI7V4Th11kVo8EjzX59RRn1MP5hjB330H6m3PsVv4ugPnW2tTb1BniUmzNnwkJtDmHzuSTMj1mkzW6cybHlsz5/bUsRbrJJb1+7yADafuTGWozeqWXxYzs+nAdIDBgwe3q49oNPglN+gvuee21G1P29bKurJNY03P5QAFmLU8qNy2/vZwgxqCR3N1W3vdnjat9uHekA2N4Ljxc+Nz8Udb2zW0dyefegq9jpz6WiLUkRMe53hwHPHg2LyOSEJ5Tn0tOTXhsdeSU9+0rdXX4vU11FNNPbup82pqqaaO3dR5DbVWTZ1XU0cNTj2O4+bE/yP+bMGreuqDsoQ6wRlwq48fUd/QNmgT/JsXtvPgeU899rxvwznA2NP3nlrhe4EHv543nAnKG9cPygzCvoNzlnCu4Y8/3ku87/A4sf+G44Z+SDgTnIv/UtFsu3j9vdon1qHh5zCgV+4BdIWuTgIbgEEJrweGZXtx9weAByAYE2jPGz36aHtaSfZKuKUgkkW6+mLxLWCEmQ0zs3xgKjCvi2MQEZFQl14JuHutmV0D/IVgiuhsd1/WlTGIiMgeXT4m4O7zgfld/b4iItKU5g6IiGQxJQERkSymJCAiksWUBEREspiSgIhIFuv2S0mbWRXwQTubHwx8ksJwejJ9FnvT57E3fR57ZMJnMcTdi5Kp2O2TQEeYWVmyK+llOn0We9PnsTd9Hntk22eh20EiIllMSUBEJItlehJ4IN0BdCP6LPamz2Nv+jz2yKrPIqPHBEREZN8y/UpARET2ISOTgJlNMbNVZrbazG5OdzzpZGaDzOxvZrbczJaZ2ffTHVO6mVnEzN4xs/9JdyzpZmZ9zexpM1tpZivM7Ph0x5ROZnZD+PdkqZk9aWaF6Y6ps2VcEgg3s78X+BowGrjAzEanN6q0qgV+4O6jgeOAq7P88wD4PrAi3UF0E3cBL7j7SOAosvhzMbMBwHVAqbuPJVjufmp6o+p8GZcESNjM3t2rgfhm9lnJ3Te6+9vh8XaCv+QD0htV+pjZQOD/AA+lO5Z0M7M+wEnAwwDuXu3un6U3qrTLBfYzs1ygF/CvNMfT6TIxCTS3mX3W/qOXyMyGAuOBN9IbSVrdCdwI1Kc7kG5gGFAFPBLeHnvIzPZPd1Dp4u4bgF8D64GNwFZ3fzG9UXW+TEwC0gwzOwB4Brje3belO550MLMzgY/dfVG6Y+kmcoEJwCx3Hw98DmTtGJqZ9SO4azAMOBzY38y+k96oOl8mJoGkNrPPJmaWR5AA5rr7s+mOJ40mAWeZ2TqC24RfMbPH0xtSWlUCle4evzJ8miApZKtTgffdvcrda4BngRPSHFOny8QkoM3sE5iZEdzzXeHut6c7nnRy9xnuPtDdhxL8f/FXd8/43/Ra4u4fAR+aWUlYdAqwPI0hpdt64Dgz6xX+vTmFLBgo7/I9hjubNrNvYhJwEbDEzBaHZbeEez2LXAvMDX9hWgtcmuZ40sbd3zCzp4G3CWbVvUMWfHtY3xgWEclimXg7SEREkqQkICKSxZQERESymJKAiEgWUxIQEcliSgIiIllMSUBEJIspCYiIZLH/D03aGoNXQNuKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you choose to program the plot, please do so within Jupyter here.\n",
    "# Make sure to explain the plot, as indicated in the question!\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([0.4, 1.0, 1.5, 1.7, 2.0, 2.1, 3.1, 3.7, 4.3, 4.9])\n",
    "n = 10\n",
    "alpha = 2.5\n",
    "beta = 4\n",
    "\n",
    "prior = np.zeros((n, 1))\n",
    "posterior = np.zeros((n, 1))\n",
    "\n",
    "for i in range(n):\n",
    "    prior[i] = alpha*(beta**alpha)*(y[i]**(-(alpha+1)))\n",
    "    posterior[i] = alpha*(beta**alpha)*(y[i]**(-(alpha)))\n",
    "\n",
    "plt.plot(range(n), y, 'b-', label=\"likelihood\")\n",
    "plt.plot(range(n), prior, 'r-', label=\"prior\")\n",
    "plt.plot(range(n), posterior, 'g-', label=\"posterior\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Limitations of the Perceptron\n",
    "\n",
    "We aim to train a perceptron to model the logic functions **OR**$(x_1,x_2)$ and **XNOR**$(x_1, x_2)$, using the set of four 2D points, $x \\in \\{(0,0)^T, (1,0)^T, (0,1)^T, (1,1)^T\\}$.\n",
    "\n",
    "In order to model **OR**$(x_1,x_2)$, the perceptron classifier must output $1$ for $x \\in \\{(1,0)^T,(0,1)^T, (1,1)^T\\}$ and output $0$ if $x = (0,0)^T$. The perceptron classifier (activation threshold of the perceptron) is represented by $f(x) =\\mathbf 1[ w^Tx + b > 0]$.\n",
    "\n",
    "Instead of using a bias vector $b$, we can augment the data by $1$ and use a linear classifier: $f(x) =\\mathbf 1[ w^T x > 0 ]$. To do this, we replace $x$ with $x$ $\\in \\{(1,1,0)^T,(1,0,1)^T, (1,1,1)^T, (1,0,0)\\}$ and $w$ with a vector in $\\mathbb{R^3}$.\n",
    "\n",
    "**a.** Using the initial weight vector $w_0 = (0,0,0)^T$ and the [perceptron algorithm](https://www.cs.cmu.edu/~avrim/ML10/lect0125.pdf), derive the $w$ that models the **OR**$(x_1,x_2)$ function. **You can do this either manually (i.e. writing out the weight updates) or by programming the algorithm in Jupyter.** Before you begin, you should normalize your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector on epoch  0  sample  1 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  2 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  3 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  4 is  [-0.29289322  0.70710678  0.        ]\n",
      "Weight vector on epoch  1  sample  1 is  [-0.29289322  0.70710678  0.        ]\n",
      "Weight vector on epoch  1  sample  2 is  [0.41421356 0.70710678 0.70710678]\n",
      "Weight vector on epoch  1  sample  3 is  [0.41421356 0.70710678 0.70710678]\n",
      "Weight vector on epoch  1  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  1 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  2 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  3 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  1 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  2 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  3 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "The final weight vector to model OR(x1,x2) with this data is:  [-0.58578644  0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "# If you choose to program your algorithm, do so here. \n",
    "# Do not use sklearn except where we used it :).\n",
    "# We have started you off. Fill in places where we've written \"YOUR CODE HERE\".\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Step 1. Initialize weight vector & define data.\n",
    "x = np.array([[1, 1, 0], [1, 0, 1], [1, 1, 1], [1, 0, 0]]) # given data\n",
    "y = np.array([1, 1, 1, 0]) # correct predictions\n",
    "w = np.array([0, 0, 0])\n",
    "\n",
    "# Normalize each sample to have norm 1.\n",
    "x = normalize(x, norm='l2')\n",
    "\n",
    "# Step 2. Activation threshold (prediction).\n",
    "def predict(sample, weights):\n",
    "    ## YOUR CODE HERE: Return the prediction (1 or 0) based on the activation threshold\n",
    "    pred = weights.T @ sample\n",
    "    \n",
    "    if pred > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    ##\n",
    "\n",
    "# Step 3. Updating weights.\n",
    "def update(w, x, y, epochs):\n",
    "    for j in range(epochs):\n",
    "        i = 0\n",
    "        for sample in x:\n",
    "            # Make prediction using the above function\n",
    "            prediction = predict(sample, w)\n",
    "            \n",
    "            ## YOUR CODE HERE: Update weights according to the link above (pg. 2)\n",
    "            # Hint: how do we usually update weights? Use the true label somewhere...\n",
    "            if y[i] != prediction and y[i] == 1:\n",
    "                w = w + sample\n",
    "            elif y[i] != prediction and y[i] == 0:\n",
    "                w = w - sample\n",
    "            ##\n",
    "            \n",
    "            i += 1\n",
    "            # This will help us see how often we make mistakes\n",
    "            print(\"Weight vector on epoch \", j, \" sample \", i, \"is \", w)\n",
    "    return w\n",
    "\n",
    "# Now, run the perceptron! Remember, you are done when the weight vector stabilizes.\n",
    "epochs = 4 # \"stop criteria\" - arbitrary, feel free to change as you see fit.\n",
    "w_new = update(w, x, y, epochs)\n",
    "print(\"The final weight vector to model OR(x1,x2) with this data is: \", w_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** We just showed that a perceptron can model **OR**$(x_1, x_2)$ successfully. Prove that a perceptron can't model **XNOR**$(x_1, x_2)$. *Hint: think about linearity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "XNOR(x1, x2) cannot be modeled by a perceptron because XNOR is not linearly seperable. If we think about four points at (0,0), (0,1), (1,0), (1,1), and graph it ,we get a square of four points. The issue arrises when we realize that after running a XNOR function on each of the four points, we realize that point (0,0) and (1,1) should have a label of 1, and (1,0), and (0,1) should have a label of 0. This means that the opposite corners of the square have the same label, which means there is no way to linearly separate these classes.\n",
    "This can be shown with inequalities. Consider point (x1, x2) with weights (w1, w2):\n",
    "\n",
    "$xnor(x1, x2): x1.w1 + x2.w2:$\n",
    "\n",
    "$xnor(0,0): 0*w1 + 0*w2 >= 0 $ (label 1)\n",
    "\n",
    "$0 >= 0 $\n",
    "\n",
    "$xnor(0,1): 0*w1 + 1*w2 < 0 $ (label 0) \n",
    "\n",
    "$w2 < 0 $\n",
    "\n",
    "$xnor(1,0): 1*w1 + 0*w2 < 0 $ (label 0)\n",
    "\n",
    "$w1 < 0 $\n",
    "\n",
    "$xnor(1,1): 1*w1 + 1*w2 >= 0 $ (label 1)\n",
    "\n",
    "$w1 + w2 >= 0 $\n",
    "\n",
    "Following this logic, for a XNOR perceptron, both w1 and w2 need to individually be less than 0, but their sum must be greater than or equal to zero. Clearly, this is impossible, making an XNOR perceptron model impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Neural Networks and the XNOR Problem\n",
    "\n",
    "So, the perceptron can't model the **XNOR**$(x_1, x_2)$ function. We now want to design a neural network (by hand) to solve the **XNOR** problem. \n",
    "\n",
    "**a.** Write the **XNOR** function in terms of the logical functions **OR**$(x_1,x_2)$, **AND**$(x_1,x_2)$, **NOR**$(x_1,x_2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The XNOR function can be written in the following terms:\n",
    "\n",
    "**XNOR**$(x_1, x_2) = x1.x2 + x1'.x2' = x1.x2 + (x1+x2)'$\n",
    "\n",
    "Or, equivalently:\n",
    "\n",
    "**XNOR**$(x_1, x_2) =$ **OR**$($**AND**$(x_1,x_2), $ **NOR**$(x_1, x_2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** We will now design a network to model this function, using the hyperbolic tangent ([$tanh$](http://reference.wolfram.com/language/ref/Tanh.html)) as the activation function in all of the nodes. The network will take two binary variables as input, and output 1 only when the inputs are both 0 or both 1.\n",
    "\n",
    "The $tanh$ function outputs [-1,+1], not our desired output of [0,1]. Thus, we have appropriately changed the OR node to take +1/-1 as inputs. Also, we have added an extra last layer to convert the final output from +1/-1 to 0/1.\n",
    "\n",
    "*Hint: assume that $tanh$ outputs -1 for any input $x\\leq -2$, +1 for any input $x\\geq 2$, and 0 for $x=0$.*\n",
    "\n",
    "<img src=\"xnor1.png\" style=\"height:130px;\"><img src=\"xnor2.png\" style=\"height:110px;\">\n",
    "\n",
    "What are the missing weights $a,b,c,d,e,f$ of the **OR**, **NAND**, **AND** and **CONVERT** subnetworks, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We can start with **OR** to solve for a:\n",
    "\n",
    "(0 or 0 = 0): $tanh(-1*2 + (-1)*2 + 1*a) = -1$\n",
    "\n",
    "$-4 + a <= -2$\n",
    "\n",
    "(0 or 1 = 1): $tanh(-1*2 + 1*2 + 1*a) = 1$\n",
    "\n",
    "$a >= 2$\n",
    "\n",
    "(1 or 0 = 1): $tanh(1*2 + (-1)*2 + 1*a) = 1$\n",
    "\n",
    "$a >= 2$\n",
    "\n",
    "(1 or 1 = 1): $tanh(1*2 + 1*2 + 1*a) = 1$\n",
    "\n",
    "$4 + a >= 2$\n",
    "\n",
    "Solving these inequalities, we find that **a = 2**\n",
    "\n",
    "We can continue with **NOR** to solve for b:\n",
    "\n",
    "(0 nor 0 = 1): $tanh(0*(-4) + 0*b + 1*2) = 1$\n",
    "\n",
    "$2 >= 2$\n",
    "\n",
    "(0 nor 1 = 0): $tanh(0*(-4) + 1*b + 1*2) = -1$\n",
    "\n",
    "$b + 2 <= -2$\n",
    "\n",
    "(1 nor 0 = 0): $tanh(1*(-4) + 0*b + 1*2) = -1$\n",
    "\n",
    "$-2 <= -2$\n",
    "\n",
    "(1 nor 1 = 0): $tanh(1*(-4) + 1*b + 1*2) = -1$\n",
    "\n",
    "$-2 + b <= -2$\n",
    "\n",
    "Solving these inequalities, we find that **b = -4**\n",
    "\n",
    "We can continue with **AND** to solve for c and d:\n",
    "\n",
    "(0 and 0 = 0): $tanh(0*4 + 0*d + 1*c) = -1$\n",
    "\n",
    "$ c <= -2$\n",
    "\n",
    "(0 and 1 = 0): $tanh(0*4 + 1*d + 1*c) = -1$\n",
    "\n",
    "$d + c <= -2$\n",
    "\n",
    "(1 and 0 = 0): $tanh(1*4 + 0*d + 1*c) = -1$\n",
    "\n",
    "$4 + c <= -2$\n",
    "\n",
    "(1 and 1 = 1): $tanh(1*4 + 1*d + 1*c) = 1$\n",
    "\n",
    "$4 + d + c >= 2$\n",
    "\n",
    "Solving these inequalities, we find that **c = -6**, and **d = 4**\n",
    "\n",
    "We can continue with **CONVERT** to solve for e and f:\n",
    "\n",
    "(-1 = 0): $tanh((-1)*f + 1*e) = 0$\n",
    "\n",
    "$-f + e = 0$\n",
    "\n",
    "(1 = 1): $tanh(1*f + 1*e) = 1$\n",
    "\n",
    "$f + e >= 2$\n",
    "\n",
    "Solving these inequalities, we find that **e = 1**, and **f = 1**\n",
    "\n",
    "These solutions could all be more negative or more positive and still solve the inequalities, but I went with the simplest solutions for all of them. \n",
    "\n",
    "This gives us all of the following values for the missing weights:\n",
    "**a = 2**, **b = -4**, **c = -6**, **d = 4**, **e = 1**, and **f = 1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
